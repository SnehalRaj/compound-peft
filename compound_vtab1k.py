# -*- coding: utf-8 -*-
"""compound_vtab1k.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cQboYtWeUw502U6DT1DB6jVJt2EKD9G1
"""

import os
import gc
import copy
import time
import random
import logging
import itertools
from datetime import datetime
import tempfile

import torch
import torch.nn as nn
import numpy as np
import pandas as pd

from PIL import Image
from sklearn.metrics import accuracy_score

from transformers import (
    AutoImageProcessor,
    AutoModel,
    AutoModelForImageClassification,
    Dinov2ForImageClassification,
    Trainer,
    TrainingArguments,
)
# For applying compound adapters
from peft import get_peft_model, CompoundConfig, TaskType
import importlib

# Datasets & utilities
from datasets import load_dataset, Dataset, DatasetDict

# -----------------------------------------------------------------------------------------
# 1. Logging Configuration
# -----------------------------------------------------------------------------------------
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# -----------------------------------------------------------------------------------------
# 2. Utilities: Seeding, memory, etc.
# -----------------------------------------------------------------------------------------
def generate_unique_seed():
    return int(time.time() * 1000) & 0xffffffff  # Use current time in ms, masked to 32 bits

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)

def free_memory():
    gc.collect()
    torch.cuda.empty_cache()

def reload_peft():
    import peft
    importlib.reload(peft)

# -----------------------------------------------------------------------------------------
# 3. DINOv2 backbone + simple classification head
# -----------------------------------------------------------------------------------------
class DINOv2ForImageClassification(nn.Module):
    def __init__(self, backbone, num_classes, dropout_rate=0.1):
        super().__init__()
        self.backbone = backbone
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(backbone.config.hidden_size, num_classes)
        self.config = backbone.config

    def forward(
        self,
        pixel_values=None,   # <--- Named arg
        labels=None,
        # if needed, also ignore text-based arguments:
        input_ids=None,
        attention_mask=None,
        **kwargs
    ):
        if pixel_values is None:
            raise ValueError("You have to specify pixel_values")

        outputs = self.backbone(pixel_values=pixel_values)
        pooled_output = outputs.last_hidden_state[:, 0]
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            loss = nn.CrossEntropyLoss()(logits, labels)
        return {"loss": loss, "logits": logits} if loss is not None else {"logits": logits}
# -----------------------------------------------------------------------------------------
# 4. VTAB streaming / partial sampling logic
#    (taken from your dinov2-large vtab pipeline)
# -----------------------------------------------------------------------------------------

def to_pil_rgb(img):
    """Convert np.ndarray or PIL.Image to a PIL RGB image."""
    if isinstance(img, np.ndarray):
        return Image.fromarray(img).convert("RGB")
    elif hasattr(img, "convert"):
        return img.convert("RGB")
    else:
        raise TypeError(f"Unrecognized image type: {type(img)}")

def _stream_n_items(dataset_path, config_name, split_name, n):
    """
    Stream exactly n items from the given split, returning a Dataset (non-streaming).
    """
    if config_name:
        streamed = load_dataset(
            dataset_path,
            config_name,
            split=split_name,
            streaming=True,
            trust_remote_code=True
        )
    else:
        streamed = load_dataset(dataset_path, split=split_name, streaming=True)

    out_list = []
    for i, item in enumerate(streamed):
        if i >= n:
            break
        out_list.append(item)
    return Dataset.from_list(out_list)

import random

def _manual_split_from_single(ds, train_n, val_n, seed=None):
    """
    Given a single-split dataset (ds), randomly sample (train_n + val_n) items,
    then partition into train (train_n) and val (val_n).
    """
    # 1) Compute how many we actually need in total
    total_n = train_n + val_n
    actual_n = min(len(ds), total_n)

    # 2) Generate a random subset of indices of size 'actual_n'
    all_indices = list(range(len(ds)))
    if seed is not None:
        random.seed(seed)
    random.shuffle(all_indices)

    subset_indices = all_indices[:actual_n]

    # 3) Partition that subset into train/val
    split_train_n = min(train_n, actual_n)
    split_val_n   = min(val_n, actual_n - split_train_n)

    train_indices = subset_indices[:split_train_n]
    val_indices   = subset_indices[split_train_n:split_train_n + split_val_n]

    # 4) Use select(...) to produce the final train_ds and val_ds
    train_ds = ds.select(train_indices)
    val_ds   = ds.select(val_indices)

    return train_ds, val_ds

def download_vtab_subset_with_test(
    dataset_name,
    dataset_path,
    train_split,
    val_split,
    test_split,
    label_key,
    train_count=800,
    val_count=200,
    config_name=None
):
    """
    Return a DatasetDict with keys "train", "val", and "test".
    - "train" is 800 examples from the official training set (or whichever split is specified).
    - "val" is 200 examples from the same (or from a dedicated val split).
    - "test" is the **full** official test set, not sub-sampled.
    """

    # Directory for caching
    local_dir = os.path.join("vtab_subsets", dataset_name)
    os.makedirs(local_dir, exist_ok=True)
    subset_path = os.path.join(local_dir, "dataset_dict_with_test")

    if os.path.isdir(subset_path):
        logger.info(f"[INFO] Loading cached VTAB subset+test for '{dataset_name}' from {subset_path}")
        return DatasetDict.load_from_disk(subset_path)

    do_streaming = (dataset_name == "dtd" or dataset_name == "sun397")
    # Some of your datasets might require streaming. Adjust as needed.
    # For example, "dtd" or "sun397" used streaming in your older code.
    # We'll keep it simple here. You can adapt as needed.

    # 1) Load or stream the train, val, test splits
    if do_streaming:
        logger.info("[INFO] Using streaming approach")
        # Train sub-sample
        train_ds_full = _stream_n_items(dataset_path, config_name, train_split, train_count + val_count)
        # Manually partition
        train_ds, val_ds = _manual_split_from_single(train_ds_full, train_count, val_count)

        # Test: load entire test
        # This might require streaming the entire test,
        # but in practice you usually won't want the entire thing in memory if it's huge...
        # We'll do normal or streaming to get it all.
        # Let's just do non-streaming fallback for demonstration:
        test_ds = load_dataset(dataset_path, config_name, split=test_split, streaming=False, trust_remote_code=True)
        # Convert to normal dataset
        test_ds = test_ds.with_format("torch")  # or .to_pandas() if needed

    else:
        logger.info("[INFO] Using normal load approach")
        if config_name:
            ds_full = load_dataset(dataset_path, config_name, trust_remote_code=True)
        else:
            ds_full = load_dataset(dataset_path)

        # Now you have ds_full with e.g. ds_full["train"], ds_full["validation"], ds_full["test"]
        # We'll sub-sample "train" for the 800/200 if val_split not there, or do partial if it is there
        splits_available = list(ds_full.keys())

        # 2) TRAIN + VAL
        if train_split in splits_available:
            full_train = ds_full[train_split]
        else:
            # fallback
            logger.warning(f"[WARNING] No split '{train_split}' found. Using first available: {splits_available[0]}")
            full_train = ds_full[splits_available[0]]
        # Sub-sample 800 + 200 from that
        train_ds, val_ds = _manual_split_from_single(full_train, train_count, val_count)

        # If there's a separate val_split you want to use for the 200, do that logic here.
        # e.g. if val_split is in ds_full, you might want to sub-sample from ds_full[val_split].
        # That depends on your exact dataset structure.

        # 3) TEST: the full official test set
        if test_split and test_split in splits_available:
            test_ds = ds_full[test_split]
        else:
            # fallback if there's only "validation" or "test"
            possible_test_keys = ["test", "validation"]
            for k in possible_test_keys:
                if k in splits_available:
                    logger.info(f"[INFO] Using split '{k}' as test for {dataset_name}")
                    test_ds = ds_full[k]
                    break
            else:
                raise ValueError(f"No official test split found for {dataset_name}. Found: {splits_available}")
    print(f'#####################val = {len(val_ds)}')
    # Build final DatasetDict
    dset_dict = DatasetDict({
        "train": train_ds,
        "val": val_ds,
        "test": test_ds,
        # "test": val_ds
    })

    dset_dict.save_to_disk(subset_path)
    logger.info(f"[INFO] Saved dataset with train/val/test for '{dataset_name}' to {subset_path}")
    return dset_dict

def preprocess_function(examples, image_processor, label_key):
    """
    Convert images to RGB and apply image_processor
    so we get pixel_values and the 'labels' field.
    """
    # Find the image column
    if "img" in examples:
        images = [to_pil_rgb(x) for x in examples["img"]]
    elif "array" in examples:
        images = [to_pil_rgb(x) for x in examples["array"]]
    elif "image" in examples:
        images = [to_pil_rgb(x) for x in examples["image"]]
    else:
        # Adjust as needed if your dataset has a different image column
        raise ValueError(f"Unknown image column in dataset. Keys: {examples.keys()}")

    inputs = image_processor(images, return_tensors="pt", padding=True)
    inputs["labels"] = examples[label_key]
    return inputs

# -----------------------------------------------------------------------------------------
# 5. Metrics for image classification
# -----------------------------------------------------------------------------------------
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc}

# -----------------------------------------------------------------------------------------
# 6. Compound adapter config setup
# -----------------------------------------------------------------------------------------
def get_compound_config(block_share, use_orthogonal, compound_pattern, compound_type,
                        num_adapters, adapter_multiplicative, rank_options):
    # For vision, PEFT might not have a dedicated "TaskType.IMAGE_CLASSIFICATION"
    # We can still pass SEQ_CLS or a custom type. Here we just pass SEQ_CLS to keep it from your code.
    return CompoundConfig(
        r=rank_options,
        compound_pattern=compound_pattern,
        compound_type=compound_type,
        block_share=block_share,
        use_orthogonal=use_orthogonal,
        num_adapters=num_adapters,
        adapter_multiplicative=adapter_multiplicative,
        task_type=TaskType.SEQ_CLS,  # or a custom enum if you have it
        #  task_type=TaskType.FEATURE_EXTRACTION,
        target_modules=["to_q", "to_v", "to_k", "query", "value", "key", "to_out.0", "add_k_proj", "add_v_proj"],
        # ^ adjust target_modules to match relevant parts of the DINOv2 model
    )

# -----------------------------------------------------------------------------------------
# 7. Single run of an experiment (similar structure to your `run_experiment`)
# -----------------------------------------------------------------------------------------
def run_experiment(
    dataset_name,
    dataset_config,
    block_share,
    use_orthogonal,
    compound_pattern,
    compound_type,
    num_adapters,
    adapter_multiplicative,
    rank_options,
    debug=False,
    seed=None,
    hyperparams=None,
):
    """
    - Load the dataset (train/val) from the VTAB subset
    - Build a DINOv2 + classification head model
    - Wrap it with a compound adapter
    - Train using Hugging Face Trainer
    - Return final eval results
    """
    if seed is not None:
        set_seed(seed)

    # 7.1. Download / load data
    train_count = 10 if debug else dataset_config.get("train_count", 800)
    val_count   = 10 if debug else dataset_config.get("val_count",   200)
    path        = dataset_config["path"]
    config_name = dataset_config.get("config", None)
    train_split = dataset_config["splits"]["train"]
    val_split   = dataset_config["splits"]["train"]
    test_split  = dataset_config["splits"].get("test", None)
    label_key   = dataset_config["label_key"]
    num_classes = dataset_config["num_classes"]
    base_lr     = dataset_config["learning_rate"]

    num_epochs  = dataset_config.get("num_epochs", 20)


    dset = download_vtab_subset_with_test(
        dataset_name=dataset_name,
        dataset_path=path,
        train_split=train_split,
        val_split=val_split,
        test_split=test_split,  # pass in the test split
        label_key=label_key,
        train_count=train_count,
        val_count=val_count,
        config_name=config_name
    )

    # 7.2. Preprocess data
    image_processor = AutoImageProcessor.from_pretrained("facebook/dinov2-large")
    def transform_func(examples):
        return preprocess_function(examples, image_processor, label_key)

    train_dataset = dset["train"].map(
        transform_func,
        batched=True,
        remove_columns=dset["train"].column_names,
        load_from_cache_file=False
    )
    train_dataset.set_format(type="torch", columns=["pixel_values", "labels"])

    eval_dataset = dset["val"].map(
        transform_func,
        batched=True,
        remove_columns=dset["val"].column_names,
        load_from_cache_file=False
    )
    eval_dataset.set_format(type="torch", columns=["pixel_values", "labels"])

    test_dataset = dset["test"].map(
        transform_func,
        batched=True,
        remove_columns=dset["test"].column_names,
        load_from_cache_file=False
    )
    test_dataset.set_format(type="torch", columns=["pixel_values", "labels"])

    # 7.3. Initialize the model (DINOv2 + classification head)
    backbone = AutoModel.from_pretrained("facebook/dinov2-large")



    base_model = DINOv2ForImageClassification(
        backbone=backbone,
        num_classes=num_classes,
        dropout_rate=0.1
    )

#     base_model =  Dinov2ForImageClassification.from_pretrained(
#     "facebook/dinov2-large", num_labels=num_classes
# )

    # 7.4. Create compound adapter config & wrap with PEFT
    peft_config = get_compound_config(
        block_share, use_orthogonal, compound_pattern, compound_type,
        num_adapters, adapter_multiplicative, rank_options
    )
    model = get_peft_model(base_model, peft_config)

    # After building or wrapping the model
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    logger.info(f"Total parameters: {total_params:,}")
    logger.info(f"Trainable parameters: {trainable_params:,}")

    # 7.5. Training arguments
    #     We'll do a quick default set, but you can pass hyperparams to override.
    default_args = dict(
        output_dir=f"./results_{dataset_name}",
        evaluation_strategy="epoch",
        lr_scheduler_type="cosine",
        save_strategy="no",
        num_train_epochs=num_epochs,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        learning_rate=base_lr,
        logging_steps=20,
        disable_tqdm=False,
        report_to="none",     # If you want to integrate wandb, set 'wandb'
        load_best_model_at_end=False,
        metric_for_best_model="accuracy",
        warmup_ratio=0.1,
        weight_decay=0.02,
        remove_unused_columns=False,
    )
    if hyperparams:
        default_args.update(hyperparams)
    if debug:
        default_args["num_train_epochs"] = 3  # or some smaller number

    training_args = TrainingArguments(**default_args)

    # 7.6. Trainer with custom best metric tracking
    class BestMetricCallback(Trainer):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.best_metric = float('-inf')
        def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix="eval"):
            result = super().evaluate(eval_dataset=eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)
            acc = result.get("eval_accuracy", float('-inf'))
            if acc > self.best_metric:
                self.best_metric = acc
            return result

    trainer = BestMetricCallback(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        compute_metrics=compute_metrics
    )

    # 7.7. Train
    logger.info(f"Starting training on dataset '{dataset_name}' with compound adapter config:")
    logger.info(f"  block_share={block_share}, use_orthogonal={use_orthogonal}, pattern={compound_pattern}, "
                f"type={compound_type}, num_adapters={num_adapters}, multiplicative={adapter_multiplicative}, rank={rank_options}")
    trainer.train()

    # 7.8. Evaluate
    final_eval_results = trainer.evaluate()
    best_acc = trainer.best_metric  # best evaluation accuracy found
    final_acc = final_eval_results.get("eval_accuracy", 0.0)

    test_results = trainer.evaluate(eval_dataset=test_dataset)
    test_acc = test_results.get("eval_accuracy", 0.0)

    logger.info(f"[{dataset_name}] Final test accuracy: {test_acc:.3%}")

    # 7.9. Cleanup
    del trainer, model
    free_memory()

    return final_eval_results, best_acc, final_acc, test_acc

# -----------------------------------------------------------------------------------------
# 8. Running multiple configurations (like the compound pipeline structure)
# -----------------------------------------------------------------------------------------
def run_experiments(
    dataset_name,
    dataset_config,
    configurations,
    num_runs=1,
    debug=False,
    hyperparams=None
):
    """
    For each config, train multiple seeds, gather results (val + test).
    """
    results = []
    for (block_share, use_orthogonal, compound_pattern, compound_type,
         num_adapters, adapter_multiplicative, rank_options) in configurations:

        config_label = (
            f"bs={block_share}_uo={use_orthogonal}_cp={compound_pattern}_ct={compound_type}_"
            f"na={num_adapters}_am={adapter_multiplicative}_ro={rank_options}"
        )
        logger.info(f"\n--- Running config: {config_label} ---")

        config_runs = []
        for run_idx in range(num_runs):
            logger.info(f"  Reloading PEFT ...")
            reload_peft()
            logger.info(f"  Freeing memory ...")
            free_memory()

            seed = generate_unique_seed()
            logger.info(f"  [Run {run_idx+1}/{num_runs}] with seed={seed}")
            final_eval, best_val_acc, final_val_acc, test_acc = run_experiment(
                dataset_name,
                dataset_config,
                block_share,
                use_orthogonal,
                compound_pattern,
                compound_type,
                num_adapters,
                adapter_multiplicative,
                rank_options,
                debug=debug,
                seed=seed,
                hyperparams=hyperparams,
            )

            config_runs.append({
                "run": run_idx + 1,
                "seed": seed,
                "best_val_acc": best_val_acc * 100,
                "final_val_acc": final_val_acc * 100,
                "test_acc": test_acc * 100,
                "eval_results": final_eval
            })

        avg_val = np.mean([r["final_val_acc"] for r in config_runs])
        std_val = np.std([r["final_val_acc"] for r in config_runs])
        avg_test = np.mean([r["test_acc"] for r in config_runs])
        std_test = np.std([r["test_acc"] for r in config_runs])

        results.append({
            "dataset": dataset_name,
            "block_share": block_share,
            "use_orthogonal": use_orthogonal,
            "compound_pattern": compound_pattern,
            "compound_type": compound_type,
            "num_adapters": num_adapters,
            "adapter_multiplicative": adapter_multiplicative,
            "rank_options": rank_options,
            "avg_val_acc": avg_val,
            "std_val_acc": std_val,
            "avg_test_acc": avg_test,
            "std_test_acc": std_test,
            "runs": config_runs,
        })

    return pd.DataFrame(results)

# -----------------------------------------------------------------------------------------
# 9. Helper: saving & loading results
# -----------------------------------------------------------------------------------------
def save_results(df, filename=None):
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"compound_adapter_results_{timestamp}.csv"
    df.to_csv(filename, index=False)
    logger.info(f"Results saved to {filename}")

def load_results(filename):
    return pd.read_csv(filename)

def analyze_results(df):
    display_columns = [col for col in df.columns if col not in ['runs_details']]
    logger.info("\nOverall Results:")
    logger.info("\n" + str(df[display_columns]))

# -----------------------------------------------------------------------------------------
# 10. Main pipeline orchestration
# -----------------------------------------------------------------------------------------
def main_vtab_experiments(
    dataset_configs,
    configurations,
    debug=False,
    num_runs=1,
    save=False
):
    """
    - `dataset_configs`: dictionary of dataset_name -> config dict
      (path, label_key, num_classes, splits, learning_rate, etc.)
    - `configurations`: list of (block_share, use_orthogonal, compound_pattern, ...) tuples
    - `debug`: if True, train/val sets are reduced drastically
    - `num_runs`: how many seeds to run per config
    - `save`: if True, results are saved to CSV
    """
    all_df = []
    for dataset_name, config in dataset_configs.items():
        logger.info(f"=== Running dataset: {dataset_name} ===")
        df = run_experiments(
            dataset_name,
            config,
            configurations,
            num_runs=num_runs,
            debug=debug,
            hyperparams=None  # or pass any custom hyperparams
        )
        analyze_results(df)
        if save:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            fname = f"compound_adapter_results_{dataset_name}_{timestamp}.csv"
            save_results(df, filename=fname)
        all_df.append(df)

    if len(all_df) > 1:
        final_df = pd.concat(all_df, ignore_index=True)
    else:
        final_df = all_df[0]

    logger.info("All experiments complete.")
    return final_df

# -----------------------------------------------------------------------------------------
# 11. Example usage
# -----------------------------------------------------------------------------------------
if __name__ == "__main__":
    # Just an example subset of your original dataset configs from the DINOv2 pipeline
    # You can add more as needed.
    vtab_dataset_configs = {
        "cifar100": {
                "path": "cifar100",
                "num_classes": 100,
                "splits": {"train": "train", "val": "val", "test":"test"},
                "label_key": "fine_label",
                "learning_rate": 8e-4,
                "train_count": 800,
                "val_count": 200,
                "num_epochs": 15,
            },
        #  "caltech101": {
        #         "path": "HuggingFaceM4/Caltech-101",
        #         "config": "with_background_category",
        #         "num_classes": 102,
        #         "splits": {"train": "train", "val": "train", "test":"test"},
        #         "label_key": "label",
        #         "learning_rate": 3e-3,
        #          "train_count": 800,
        #         "val_count": 200,
        #     },
            # "flower102": {
            #     "path": "Donghyun99/Oxford-Flower-102",
            #     "num_classes": 102,
            #     "splits": {"train": "train", "val": "train", "test":"test"},
            #     "label_key": "label",
            #     "learning_rate": 2e-6,
            #      "train_count": 800,
            #     "val_count": 200,
            # },
            # "pets": {
            #     "path": "timm/oxford-iiit-pet",
            #     "num_classes": 37,
            #     "splits": {"train": "train", "val": "train", "test":"test" },
            #     "label_key": "label",
            #     "learning_rate": 2e-3,
            #      "train_count": 800,
            #     "val_count": 200,
            #      "num_epochs": 5,
            # },
            # "svhn": {
            #     "path": "ufldl-stanford/svhn",
            #     "config": "cropped_digits",
            #     "num_classes": 10,
            #     "splits": {"train": "train", "val": "train", "test":"test"},
            #     "label_key": "label",
            #     "learning_rate": 2e-3,
            #     "train_count": 800,
            #     "val_count": 200,
            #     "num_epochs": 10,
            # },
        # "dtd": {
        #     "path": "cansa/Describable-Textures-Dataset-DTD",
        #     "num_classes": 47,
        #     "splits": {"train": "train", "val": "validation", "test":"test"},
        #     "label_key": "label",
        #     "learning_rate": 3e-7,
        #     "train_count": 800,
        #     "val_count": 200
        # },
        # "sun397": {
        #     "path": "tanganke/sun397",
        #     "num_classes": 397,
        #     "splits": {"train": "train", "val": "validation"},
        #     "label_key": "label",
        #     "learning_rate": 6e-6,
        #     "train_count": 800,
        #     "val_count": 200
        # },

    # -----------------------------------------------------------------------------------------
    # Specialized
    # -----------------------------------------------------------------------------------------

    # "eurosat": {
    #     "path": "tanganke/eurosat",
    #     "num_classes": 10,
    #     "splits": {"train": "train", "val": "train", "test":"test"},
    #     "label_key": "label",
    #     "learning_rate": 1e-5,
    #     "train_count": 800,
    #     "val_count": 200,
    #      "num_epochs": 60,
    # },
    # "resic45": {
    #     "path": "timm/resisc45",
    #     "num_classes": 45,
    #     "splits": {"train": "train", "val": "train", "test":"test"},
    #     "label_key": "label",
    #     "learning_rate": 5e-5,
    #     "train_count": 800,
    #     "val_count": 200,
    #      "num_epochs": 60,
    # },
    # "patchcamelyon": {
    #     "path": "1aurent/PatchCamelyon",
    #     "num_classes": 2,
    #     "splits": {"train": "train", "val": "train", "test":"test"},
    #     "label_key": "label",
    #     "learning_rate": 9e-5,
    #     "train_count": 800,
    #     "val_count": 200,
    #      "num_epochs": 60,
    # },

     # -----------------------------------------------------------------------------------------
    # Structured
    # -----------------------------------------------------------------------------------------
    #  "dmlab": {
    #     "path": "dpdl-benchmark/dmlab",
    #     "num_classes": 6,
    #     "splits": {"train": "train", "val": "train", "test":"test"},
    #     "label_key": "label",
    #     "learning_rate": 9e-4,
    #     "train_count": 800,
    #     "val_count": 200,
    #      "num_epochs": 5,
    # },


    }

    # Example: two simple configurations
    # (block_share, use_orthogonal, compound_pattern, compound_type, num_adapters, adapter_multiplicative, rank_options)
    configurations_for_experiment = [
        # (False, True,  ['comp_1'],      'comp', 1, True, 16),
        (False, True,  ['comp_1', 'comp_2'],      'comp', 1, True, 3),
        # (False, True,  ['comp_1', 'comp_2', 'comp_3'], 'comp', 1, True, 3),
    ]

    # Run
    final_results_df = main_vtab_experiments(
        dataset_configs=vtab_dataset_configs,
        configurations=configurations_for_experiment,
        debug=False,   # set True to drastically reduce dataset sizes
        num_runs=1,    # how many seeds per config
        save=False,     # whether to save results to CSV
    )

    logger.info("Final aggregated results:")
    analyze_results(final_results_df)

